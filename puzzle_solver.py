# -*- coding: utf-8 -*-
"""71.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v163Gn61ZTOFcZJjhyIEo3SBBI4y1IRF
"""

pip install ecdsa base58 pycryptodome

# Google Colab detection and setup
try:
    from google.colab import drive
    USING_COLAB = True
    # Install required packages in Colab
    import subprocess
    print("Installing required packages...")
    # Removed "cupy-cuda12x" as it's not used by this CPU-based script
    subprocess.check_call(["pip", "install", "pycryptodome", "ecdsa", "base58"])
    print("✅ Packages installed successfully")
except ImportError:
    USING_COLAB = False

import os
import pickle
import logging
import random
import multiprocessing as mp
import time
import math
import struct
import statistics
import numpy as np
from collections import namedtuple, defaultdict
from datetime import datetime

# --- ECDSA & Hashing ---
from ecdsa import SECP256k1, SigningKey
import hashlib
import base58
from math import gcd
from typing import List, Tuple, Optional, Dict

# --- RIPEMD-160 Handling ---
try:
    hashlib.new("ripemd160")
    HASHLIB_RIPEMD160_AVAILABLE = True
except ValueError:
    HASHLIB_RIPEMD160_AVAILABLE = False
    try:
        from Crypto.Hash import RIPEMD160
        CRYPTO_RIPEMD160_AVAILABLE = True
        print("✅ Using RIPEMD-160 from pycryptodome")
    except ImportError:
        CRYPTO_RIPEMD160_AVAILABLE = False
        print("❌ RIPEMD-160 not available! Bitcoin addresses can't be calculated correctly.")
        exit(1)


def ripemd160_hash(data):
    if HASHLIB_RIPEMD160_AVAILABLE:
        return hashlib.new("ripemd160", data).digest()
    elif CRYPTO_RIPEMD160_AVAILABLE:
        ripemd = RIPEMD160.new()
        ripemd.update(data)
        return ripemd.digest()
    else:
        # This case should be prevented by the exit(1) above if both are unavailable.
        raise ValueError("No RIPEMD-160 implementation available.")

# --- Configuration for Puzzle  ---
PUZZLE_ID = 71
TARGET_ADDRESS = "1PWo3JeB9jrGwfHDNpdGK54CRas7fsVzXU"
RANGE_START = int("0000000000000000000000000000000000000000000000400000000000000000", 16)
RANGE_END   = int("00000000000000000000000000000000000000000000007fffffffffffffffff", 16)

# --- Statistical Evaluation Config
FIXED_SAMPLE_COUNT = 1000000    # Adjusted for potentially more thorough search with more RAM
HAMMING_THRESHOLD = 52         # Stricter threshold
STAT_SCORE_WEIGHTS = {
    'min_hamming': 0.55,        # Slightly reduced from 0.65
    'density_good': 0.25,       # Slightly reduced from 0.30
    'avg_hamming': 0.05,        # Unchanged
    'std_dev_penalty': 0.02,    # Added penalty for high variability (was 0.0)
    'hamming_gradient': 0.08,   # NEW: weight for gradient analysis
    'bit_significance': 0.03,   # NEW: weight for bit position significance
    'pattern_consistency': 0.02 # NEW: weight for pattern consistency
}

# --- File/Directory Config ---
if USING_COLAB:
    MOUNTPOINT = '/content/gdrive'
    DATA_DIR = os.path.join(MOUNTPOINT, 'My Drive', f'puzzle{PUZZLE_ID}_1Sampling') # Changed dir name slightly
    try:
        drive.mount(MOUNTPOINT, force_remount=True)
        print(f"✅ Google Drive mounted successfully")
        os.makedirs(DATA_DIR, exist_ok=True)
        print(f"✅ Data directory created: {DATA_DIR}")
    except Exception as e:
        print(f"⚠️ Failed to mount Google Drive: {e}")
        print("Falling back to local storage")
        DATA_DIR = os.path.join(os.getcwd(), f'puzzle{PUZZLE_ID}_1Sampling')
        os.makedirs(DATA_DIR, exist_ok=True)
else:
    DATA_DIR = os.path.join(os.getcwd(), f'puzzle{PUZZLE_ID}_1Sampling')
    os.makedirs(DATA_DIR, exist_ok=True)

CHECKPOINT_FILE = os.path.join(DATA_DIR, "statistical_checkpoint_sampling.pkl")
LOG_FILE = os.path.join(DATA_DIR, "statistical_solver_sampling.log")

# --- Logging Setup ---
# Remove existing handlers before adding new ones, especially if re-running in a notebook
root_logger = logging.getLogger()
for handler in root_logger.handlers[:]:
    root_logger.removeHandler(handler)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(), logging.FileHandler(LOG_FILE)]
)
logging.Formatter.converter = time.gmtime # Use UTC for logs

# --- Decode Target Address to HASH160 ---
try:
    TARGET_HASH160_BYTES = base58.b58decode_check(TARGET_ADDRESS)[1:]
    TARGET_HASH160_INT = int.from_bytes(TARGET_HASH160_BYTES, 'big') # Not used elsewhere but good to have
    logging.info(f"Target Address: {TARGET_ADDRESS}")
    logging.info(f"Target HASH160: {TARGET_HASH160_BYTES.hex()}")
except Exception as e:
    logging.critical(f"Could not decode target address {TARGET_ADDRESS}: {e}")
    exit(1)

# --- Data Structures ---
Range = namedtuple('Range', ['start', 'end'])
RangeStatScore = namedtuple('RangeStatScore', [
    'range_id', 'score', 'min_hamming', 'avg_hamming',
    'std_dev_hamming', 'density_good', 'best_key_in_range', 'num_samples',
    'entropy', 'hamming_gradient', 'bit_significance', 'pattern_consistency'
])

class SearchState:
    def __init__(self, puzzle_id=PUZZLE_ID, target_address=TARGET_ADDRESS,
                 range_start=RANGE_START, range_end=RANGE_END):
        self.puzzle_id = puzzle_id
        self.target_address = target_address
        self.range_start = int(range_start)
        self.range_end = int(range_end)

        # Add bit-structure awareness for Bitcoin puzzles
        self.puzzle_bit_length = puzzle_id
        self.leading_zeros_bits = 256 - puzzle_id
        self.hex_prefix = '0' * (self.leading_zeros_bits // 4)

        # Validate range matches expected puzzle structure
        expected_start = 1 << (puzzle_id - 1) if puzzle_id > 1 else 1
        expected_end = (1 << puzzle_id) - 1

        if self.range_start != expected_start or self.range_end != expected_end:
            logging.warning(f"Range [{hex(self.range_start)}, {hex(self.range_end)}] doesn't match "
                          f"expected [{hex(expected_start)}, {hex(expected_end)}] for puzzle {puzzle_id}")

        if self.range_end <= self.range_start:
            logging.critical("Range end must be greater than range start. Exiting.")
            raise ValueError("Range end must be greater than range start")
        self.range_size = self.range_end - self.range_start

        # Progressive refinement strategy (NEW)
        self.granularity_level = 0  # 0=10%, 1=5%, 2=1%, 3=0.1%, 4=0.01%, etc.
        self.segments_per_level = [10, 20, 100, 1000, 10000, 100000]  # 10%, 5%, 1%, 0.1%, 0.01%, 0.001%
        self.rounds_at_current_granularity = 0  # Tracks rounds at current level
        self.total_rounds_completed = 0  # Total rounds across all granularity levels
        self.granularity_history = []  # Track granularity progression
        self.min_hamming_per_level = []  # Best scores at each granularity level

        # Adaptive threshold based on range size
        if self.range_size > 2**100:  # Full or very large range
            self.thorough_exploration_threshold = 800000
        elif self.range_size > 2**80:  # Large range
            self.thorough_exploration_threshold = 600000
        else:  # Smaller range
            self.thorough_exploration_threshold = 500000

        logging.info(f"Using thorough exploration threshold: {self.thorough_exploration_threshold:,} samples based on range size")

        self.cycle = 1

        # Basic state tracking
        self.best_score = 161  # Best Hamming distance found globally
        self.best_key = None   # Private key corresponding to best_score
        self.last_improvement_cycle = 0  # Cycle number of last best_score improvement

        # Exploration properties
        self.current_exploration_mode = "breadth"  # Always "breadth" for this approach
        self.methodical_exploration_phase = 0  # Current segment index being explored
        self.thoroughly_explored_ranges = set()  # Set of range_id strings that are thoroughly explored
        self.range_samples_count = {}  # Track samples taken from each range
        self.promising_ranges_queue = []  # Queue of promising ranges to explore

        # Base ranges for each level
        self.base_ranges = []  # Current level's base ranges
        self.previously_explored_keys = set()  # NEW: Track keys we've already checked
        self.previously_explored_ranges = set()  # NEW: Track entire ranges we've already checked
        self.exploration_memory = {}  # NEW: Maps granularity level -> set of explored chunk indices

        # Subrange exploration for ultra-fine levels
        self.subrange_granularity = 0.1  # Start with 0.1% of segment for fine exploration
        self.explored_subranges = {}  # Maps range_id -> set of (start_pct, end_pct) tuples

        # Result tracking
        self.range_scores = {}  # Stores RangeStatScore objects
        self.best_range_history = []  # List of best range_id strings from recent cycles

        # Adaptive search properties
        self.adaptive_hamming_threshold = HAMMING_THRESHOLD
        self.region_entropy_map = {}  # Maps region strings to entropy values

        # Memory-based sampling tracking
        self.sampled_keys = {}  # Maps range_id_str to sets of sampled keys (limited storage)
        self.sampled_regions = {}  # Maps range_id_str to sampling metadata
        self.use_complementary_sampling = False  # Flag for avoiding previous samples
        self.sampling_offsets = {}  # Different offsets for different rounds

        # Historical puzzle solution data for strategic sampling
        self.historical_key_percentages = [
            0.72, 6.94, 13.67, 17.77, 23.67, 32.63, 45.89, 46.11,
            65.62, 65.71, 68.48, 69.50, 70.06, 75.13, 82.56, 89.98,
            95.01, 96.90
        ]

    def initialize_base_ranges(self):
        """Initialize the base ranges for current granularity level"""
        total_range_size = self.range_end - self.range_start
        num_segments = self.segments_per_level[self.granularity_level]
        self.base_ranges = []

        logging.info(f"Initializing {num_segments} base ranges for granularity level {self.granularity_level}")

        for i in range(num_segments):
            chunk_start = self.range_start + (i * total_range_size // num_segments)
            chunk_end = self.range_start + ((i + 1) * total_range_size // num_segments)
            if i == num_segments - 1:  # Ensure the last chunk goes exactly to range_end
                chunk_end = self.range_end
            self.base_ranges.append(Range(chunk_start, chunk_end))

        # Initialize range samples count for each base range
        for r in self.base_ranges:
            range_id = str((r.start, r.end))
            if range_id not in self.range_samples_count:
                self.range_samples_count[range_id] = 0

        # Initialize exploration memory for this level if needed
        if self.granularity_level not in self.exploration_memory:
            self.exploration_memory[self.granularity_level] = set()

        logging.info(f"Initialized {num_segments} base ranges for granularity level {self.granularity_level}")
        return self.base_ranges

    def should_advance_granularity(self):
        """Check if we should advance to next granularity level"""
        # Simple rule: advance after completing enough rounds
        if (self.rounds_at_current_granularity >= 5 and
            self.granularity_level < len(self.segments_per_level) - 1):
            return True
        return False

    def advance_granularity(self):
        """Advance to next granularity level"""
        # Store the best score achieved at this level
        self.min_hamming_per_level.append(self.best_score)

        # Store current granularity info
        granularity_percentage = 100.0 / self.segments_per_level[self.granularity_level]
        self.granularity_history.append(granularity_percentage)

        # Move to next level
        self.granularity_level += 1
        level_name = "10%" if self.granularity_level == 0 else \
                    "5%" if self.granularity_level == 1 else \
                    "1%" if self.granularity_level == 2 else \
                    f"0.{'0' * (self.granularity_level-3)}1%"

        logging.info(f"✨ Advancing to granularity level {self.granularity_level}: {level_name}")

        # Reset exploration state for the new level
        self.rounds_at_current_granularity = 0
        self.methodical_exploration_phase = 0
        self.thoroughly_explored_ranges.clear()

        # For ultra-fine levels (beyond 1%), initialize subrange exploration
        if self.granularity_level > 2:
            self.subrange_granularity = 0.5  # Start with 0.5% instead of 0.1%
            self.explored_subranges.clear()
            logging.info(f"Ultra-fine level initialized with subrange granularity: {self.subrange_granularity*100:.1f}%")

        # Initialize new base ranges
        self.initialize_base_ranges()

        return self.granularity_level


    def mark_range_explored(self, range_obj, samples_taken):
        """Mark a range as having been sampled, and potentially as thoroughly explored"""
        range_id = str((range_obj.start, range_obj.end))

        # Update sample count
        if range_id not in self.range_samples_count:
            self.range_samples_count[range_id] = 0
        self.range_samples_count[range_id] += samples_taken

        # Check if now thoroughly explored
        if self.range_samples_count[range_id] >= self.thorough_exploration_threshold:
            if range_id not in self.thoroughly_explored_ranges:
                self.thoroughly_explored_ranges.add(range_id)

                # NEW: Also add to exploration memory for this granularity level
                # Find the index of this range in the base_ranges list
                for i, base_range in enumerate(self.base_ranges):
                    if str((base_range.start, base_range.end)) == range_id:
                        self.exploration_memory[self.granularity_level].add(i)
                        break

                logging.info(f"Range {range_id} marked as thoroughly explored with {self.range_samples_count[range_id]:,} samples")
                return True  # Newly marked as thoroughly explored
        return False  # Not thoroughly explored or already marked

    def get_next_unexplored_range(self):
        """
        Returns the next base range that hasn't been thoroughly explored yet.
        CLEAN VERSION: No stagnation handling, just systematic progression.
        """
        if not self.base_ranges:
            self.initialize_base_ranges()

        # Start from current phase and look for unexplored ranges
        num_segments = len(self.base_ranges)
        starting_phase = self.methodical_exploration_phase

        for i in range(num_segments):
            phase_idx = (starting_phase + i) % num_segments
            range_to_check = self.base_ranges[phase_idx]
            range_id = str((range_to_check.start, range_to_check.end))

            if range_id not in self.thoroughly_explored_ranges:
                # Advance the phase pointer for next call
                self.methodical_exploration_phase = (phase_idx + 1) % num_segments
                logging.info(f"Selected range {phase_idx}/{num_segments} at granularity level {self.granularity_level}")
                return range_to_check

        # If we get here, all ranges at this level have been thoroughly explored
        logging.info(f"All {num_segments} ranges at granularity level {self.granularity_level} are thoroughly explored")
        return None

    def get_exploration_progress(self):
        """Returns the exploration progress as a percentage"""
        if not self.base_ranges:
            return 0.0

        explored_count = len(self.thoroughly_explored_ranges)
        total_segments = len(self.base_ranges)
        return (explored_count / total_segments) * 100.0

    def record_sampled_keys(self, range_id_str, keys, max_to_store=10000):
        """Record a sample of keys that were evaluated in a given range"""
        if range_id_str not in self.sampled_keys:
            self.sampled_keys[range_id_str] = set()

        # NEW: Filter out keys we've already checked before adding to memory
        new_keys = set(k for k in keys if k not in self.previously_explored_keys)

        # Store a subset of keys to limit memory usage
        if len(new_keys) <= max_to_store:
            self.sampled_keys[range_id_str].update(new_keys)
            # Add to global memory of explored keys
            self.previously_explored_keys.update(new_keys)
        else:
            # Store a random subset
            sample_to_store = random.sample(list(new_keys), max_to_store)
            self.sampled_keys[range_id_str].update(sample_to_store)
            # Add to global memory of explored keys
            self.previously_explored_keys.update(sample_to_store)

        # Record sampling metadata
        if range_id_str not in self.sampled_regions:
            self.sampled_regions[range_id_str] = {
                "rounds": set(),
                "total_samples": 0,
                "last_cycle": 0
            }

        self.sampled_regions[range_id_str]["rounds"].add(self.total_rounds_completed + 1)
        self.sampled_regions[range_id_str]["total_samples"] += len(keys)
        self.sampled_regions[range_id_str]["last_cycle"] = self.cycle

    def to_dict(self):
        """Convert state to dictionary for serialization"""
        # Convert explored_regions set of Range objects to list of dicts
        explored_regions_serializable = [r._asdict() for r in self.explored_regions]

        # Serialize the precision-based exploration structures
        serialized_explored_subranges = {}
        for k, v in self.explored_subranges.items():
            serialized_explored_subranges[k] = list(v)  # Convert set to list for serialization

        # Convert base_ranges to serializable form
        base_ranges_serializable = [r._asdict() for r in self.base_ranges] if self.base_ranges else []

        # Convert previously_explored_keys to list for serialization
        previously_explored_keys_list = list(self.previously_explored_keys)

        # Convert exploration_memory to serializable form
        serialized_exploration_memory = {}
        for k, v in self.exploration_memory.items():
            serialized_exploration_memory[str(k)] = list(v)

        return {
            # Basic state
            'puzzle_id': self.puzzle_id,
            'target_address': self.target_address,
            'range_start': self.range_start,
            'range_end': self.range_end,
            'cycle': self.cycle,
            'best_score': self.best_score,
            'best_key': self.best_key,
            'last_improvement_cycle': self.last_improvement_cycle,

            # Progressive refinement properties
            'granularity_level': self.granularity_level,
            'segments_per_level': self.segments_per_level,
            'rounds_at_current_granularity': self.rounds_at_current_granularity,
            'total_rounds_completed': self.total_rounds_completed,
            'granularity_history': self.granularity_history,
            'min_hamming_per_level': self.min_hamming_per_level,

            # Exploration properties
            'current_exploration_mode': self.current_exploration_mode,
            'methodical_exploration_phase': self.methodical_exploration_phase,
            'thoroughly_explored_ranges': list(self.thoroughly_explored_ranges),
            'base_ranges': base_ranges_serializable,
            'range_samples_count': self.range_samples_count,
            'previously_explored_keys': previously_explored_keys_list,
            'previously_explored_ranges': list(self.previously_explored_ranges),
            'exploration_memory': serialized_exploration_memory,

            # Subrange exploration
            'subrange_granularity': self.subrange_granularity,
            'explored_subranges': serialized_explored_subranges,

            # Result tracking
            'range_scores': {str(k): v._asdict() for k, v in self.range_scores.items()},
            'best_range_history': self.best_range_history,

            # Adaptive search properties
            'adaptive_hamming_threshold': self.adaptive_hamming_threshold,
            'region_entropy_map': self.region_entropy_map,

            # Memory-based sampling tracking
            'sampled_keys': {k: list(v) for k, v in self.sampled_keys.items()},
            'sampled_regions': self.sampled_regions,
            'use_complementary_sampling': self.use_complementary_sampling,
            'sampling_offsets': self.sampling_offsets
        }

    @classmethod
    def from_dict(cls, data):
        """Create state object from serialized dictionary"""
        state = cls(
            puzzle_id=data.get('puzzle_id', PUZZLE_ID),
            target_address=data.get('target_address', TARGET_ADDRESS),
            range_start=data.get('range_start', RANGE_START),
            range_end=data.get('range_end', RANGE_END)
        )

        # Basic state
        state.cycle = data.get('cycle', 1)
        state.best_score = data.get('best_score', 161)
        state.best_key = data.get('best_key', None)
        state.last_improvement_cycle = data.get('last_improvement_cycle', 0)

        # Progressive refinement properties
        state.granularity_level = data.get('granularity_level', 0)
        state.segments_per_level = data.get('segments_per_level', [10, 20, 100, 1000, 10000])
        state.rounds_at_current_granularity = data.get('rounds_at_current_granularity', 0)
        state.total_rounds_completed = data.get('total_rounds_completed', 0)
        state.granularity_history = data.get('granularity_history', [])
        state.min_hamming_per_level = data.get('min_hamming_per_level', [])

        # Exploration properties
        state.current_exploration_mode = data.get('current_exploration_mode', "breadth")
        state.methodical_exploration_phase = data.get('methodical_exploration_phase', 0)
        state.thoroughly_explored_ranges = set(data.get('thoroughly_explored_ranges', []))

        # Restore base ranges
        base_ranges_data = data.get('base_ranges', [])
        state.base_ranges = []
        for r_dict in base_ranges_data:
            if isinstance(r_dict, dict) and 'start' in r_dict and 'end' in r_dict:
                state.base_ranges.append(Range(r_dict['start'], r_dict['end']))

        # Restore range samples count
        state.range_samples_count = data.get('range_samples_count', {})

        # Restore previously explored keys and ranges
        state.previously_explored_keys = set(data.get('previously_explored_keys', []))
        state.previously_explored_ranges = set(data.get('previously_explored_ranges', []))

        # Restore exploration memory
        serialized_memory = data.get('exploration_memory', {})
        state.exploration_memory = {}
        for k_str, v_list in serialized_memory.items():
            try:
                k = int(k_str)
                state.exploration_memory[k] = set(v_list)
            except ValueError:
                state.exploration_memory[k_str] = set(v_list)

        # Subrange exploration
        state.subrange_granularity = data.get('subrange_granularity', 0.1)

        # Restore explored subranges
        serialized_subranges = data.get('explored_subranges', {})
        state.explored_subranges = {}
        for k, v in serialized_subranges.items():
            state.explored_subranges[k] = set(tuple(item) for item in v)

        # Result tracking
        # Deserialize range_scores
        deserialized_scores = {}
        raw_scores = data.get('range_scores', {})
        for k_str, v_dict in raw_scores.items():
            try:
                deserialized_scores[k_str] = RangeStatScore(**v_dict)
            except Exception as e:
                logging.warning(f"Could not deserialize range_score for key {k_str}: {e}")
        state.range_scores = deserialized_scores

        state.best_range_history = data.get('best_range_history', [])

        # Deserialize explored_regions
        explored_regions_data = data.get('explored_regions', [])
        state.explored_regions = {Range(**r_dict) for r_dict in explored_regions_data}

        # Adaptive search properties
        state.adaptive_hamming_threshold = data.get('adaptive_hamming_threshold', HAMMING_THRESHOLD)
        state.region_entropy_map = data.get('region_entropy_map', {})

        # Memory-based sampling tracking
        serialized_sampled_keys = data.get('sampled_keys', {})
        state.sampled_keys = {}
        for k, v in serialized_sampled_keys.items():
            state.sampled_keys[k] = set(v)

        state.sampled_regions = data.get('sampled_regions', {})
        state.use_complementary_sampling = data.get('use_complementary_sampling', False)
        state.sampling_offsets = data.get('sampling_offsets', {})

        return state

# Checkpoint helpers
def save_checkpoint(state, filename=CHECKPOINT_FILE):
    try:
        with open(filename, 'wb') as f:
            pickle.dump(state.to_dict(), f)
        logging.info(f"💾 Checkpoint saved to {filename}")
    except Exception as e:
        logging.error(f"Error saving checkpoint: {e}")

def load_checkpoint(filename=CHECKPOINT_FILE):
    if os.path.exists(filename):
        try:
            with open(filename, 'rb') as f:
                data = pickle.load(f)
            logging.info("✅ Checkpoint loaded successfully.")
            return SearchState.from_dict(data)
        except Exception as e:
            logging.error(f"Error loading checkpoint: {e}. Starting fresh.")
            # Consider backing up corrupted checkpoint
            try:
                os.rename(filename, filename + ".corrupted." + datetime.now().strftime("%Y%m%d%H%M%S"))
                logging.info(f"Corrupted checkpoint backed up to {filename}.corrupted...")
            except OSError:
                logging.error("Could not back up corrupted checkpoint.")
    else:
        logging.info("No checkpoint found. Starting fresh.")
    return SearchState()

# Core Bitcoin & Hamming
def priv_to_hash160_bytes(k_int):
    if not isinstance(k_int, int): k_int = int(k_int) # Ensure k is an int
    # SECP256k1.order is the upper bound for private keys (exclusive)
    if k_int <= 0 or k_int >= SECP256k1.order:
        return None

    pk_bytes = k_int.to_bytes(32, 'big')
    sk = SigningKey.from_string(pk_bytes, curve=SECP256k1)
    vk = sk.get_verifying_key()

    # Compressed public key
    # Determine prefix based on Y coordinate parity
    y_coord = vk.pubkey.point.y()
    prefix = b'\x02' if y_coord % 2 == 0 else b'\x03'
    x_coord_bytes = vk.pubkey.point.x().to_bytes(32, 'big')
    compressed_pub_key = prefix + x_coord_bytes

    # Hashing: SHA256 then RIPEMD160
    sha256_hash = hashlib.sha256(compressed_pub_key).digest()
    hash160_result = ripemd160_hash(sha256_hash)
    return hash160_result

def priv_to_address(k_int):
    h160_bytes = priv_to_hash160_bytes(k_int)
    if h160_bytes is None: return None
    # Add version byte (0x00 for mainnet P2PKH)
    versioned_hash160 = b'\x00' + h160_bytes
    # Base58Check encode
    address = base58.b58encode_check(versioned_hash160).decode('ascii')
    return address

def hamming_distance(bytes_a, bytes_b):
    if len(bytes_a) != len(bytes_b):
        raise ValueError("Byte strings must have the same length for Hamming distance.")
    distance = 0
    for byte1, byte2 in zip(bytes_a, bytes_b):
        xor_result = byte1 ^ byte2
        distance += bin(xor_result).count('1')
    return distance

def evaluate_key_hamming(k, target_hash160_bytes):
    h160 = priv_to_hash160_bytes(k)
    if h160 is None: return 161 # Max distance + 1 for invalid keys
    return hamming_distance(h160, target_hash160_bytes)

def update_adaptive_thresholds(state):
    """
    Updates the adaptive hamming threshold based on the current best score and new metrics.
    Makes thresholds more strict as better keys are found.
    """
    # For hash160 mode (160 bits max)
    base_margin = max(10, int(state.best_score * 0.25))

    # If we have range scores with the new metrics, use them to adjust the threshold
    has_new_metrics = False
    significant_bit_patterns = False
    strong_pattern_consistency = False
    low_gradient = False

    # Check the top 5 scoring ranges for the presence of significant metrics
    top_ranges = sorted(state.range_scores.values(), key=lambda x: x.score, reverse=True)[:5]
    for rs in top_ranges:
        if hasattr(rs, 'bit_significance') and rs.bit_significance > 0.6:
            significant_bit_patterns = True
            has_new_metrics = True

        if hasattr(rs, 'pattern_consistency') and rs.pattern_consistency > 0.7:
            strong_pattern_consistency = True
            has_new_metrics = True

        if hasattr(rs, 'hamming_gradient') and rs.hamming_gradient < 0.001:
            low_gradient = True
            has_new_metrics = True

    # Adjust margin based on new metrics if available
    margin_adjustment = 0
    if has_new_metrics:
        if significant_bit_patterns:
            margin_adjustment -= 2  # Tighter threshold if we have significant bit patterns
        if strong_pattern_consistency:
            margin_adjustment -= 2  # Tighter threshold if we have strong pattern consistency
        if low_gradient:
            margin_adjustment -= 1  # Tighter threshold if we have low gradient (flat region)

    # Apply the adjusted margin
    adjusted_margin = max(5, base_margin + margin_adjustment)

    state.adaptive_hamming_threshold = min(
        HAMMING_THRESHOLD,  # Don't exceed original constant
        max(state.best_score + adjusted_margin, 20)  # At least 20 to ensure some candidates
    )

    logging.info(f"Updated hamming threshold to {state.adaptive_hamming_threshold} (best score: {state.best_score}, adjustment: {margin_adjustment})")

def analyze_bit_patterns(hash160_bytes, target_bytes):
    """
    Performs a detailed analysis of bit patterns in the HASH160 value,
    identifying which bit positions most consistently match the target.
    """
    # Count of matches for each bit position
    bit_match_counts = [0] * 160

    # Initialize bit matrices
    target_bits = []
    hash_bits = []

    # Extract individual bits
    for byte_idx in range(len(target_bytes)):
        for bit_idx in range(8):
            target_bit = (target_bytes[byte_idx] >> (7 - bit_idx)) & 1
            hash_bit = (hash160_bytes[byte_idx] >> (7 - bit_idx)) & 1

            target_bits.append(target_bit)
            hash_bits.append(hash_bit)

            if hash_bit == target_bit:
                bit_idx_global = byte_idx * 8 + bit_idx
                if bit_idx_global < 160:  # Ensure we're within 160 bits
                    bit_match_counts[bit_idx_global] += 1

    # Identify most significant matching bits (top 25%)
    sorted_bits = sorted(enumerate(bit_match_counts), key=lambda x: x[1], reverse=True)
    top_bits = sorted_bits[:40]  # Top 25% of 160 bits

    # Calculate match percentage for top bits
    match_percentage = sum(count for _, count in top_bits) / (40 * len(hash_bits)) if hash_bits else 0

    # Return both the match percentage and the indices of the significant bits
    return {
        'match_percentage': match_percentage,
        'significant_bits': [idx for idx, _ in top_bits]
    }

def allocate_samples_based_on_weights(state, ranges_to_evaluate, total_samples_available):
    """
    Allocates sampling budget based on progressive refinement strategy.
    Avoids redundant sampling of keys that have been previously explored.
    Enhanced with bit-pattern awareness and historical data.
    """
    # Base sample counts by granularity level
    base_samples_by_level = [
        1000000,  # Level 0 (10%): 1M samples
        800000,   # Level 1 (5%): 800K samples
        500000,   # Level 2 (1%): 500K samples
        100000,   # Level 3 (0.1%): 100K samples
        50000,    # Level 4 (0.01%): 50K samples
        10000     # Level 5+ (0.001% and finer): 10K samples
    ]

    level_idx = min(state.granularity_level, len(base_samples_by_level) - 1)
    BASE_SAMPLES = base_samples_by_level[level_idx]
    MIN_SAMPLES = max(1000, BASE_SAMPLES // 10)

    # Calculate historical bias weights based on solved puzzle locations
    historical_bins = [0] * 10  # 10 bins for 0-10%, 10-20%, etc.
    for pct in state.historical_key_percentages:
        bin_idx = min(int(pct / 10), 9)
        historical_bins[bin_idx] += 1

    # Create weights (higher weight for less explored bins)
    max_count = max(historical_bins) if max(historical_bins) > 0 else 1
    bin_weights = []
    for count in historical_bins:
        # Inverse frequency weight - unexplored areas get higher weight
        weight = max_count / (count + 1)
        bin_weights.append(weight)

    # Normalize weights
    total_weight = sum(bin_weights)
    normalized_bin_weights = [w / total_weight for w in bin_weights]

    samples_per_range = []

    for r in ranges_to_evaluate:
        range_size = r.end - r.start
        range_id_str = str((r.start, r.end))

        # Calculate which historical bin this range falls into
        range_start_pct = ((r.start - state.range_start) / state.range_size) * 100
        range_end_pct = ((r.end - state.range_start) / state.range_size) * 100
        range_mid_pct = (range_start_pct + range_end_pct) / 2
        bin_idx = min(int(range_mid_pct / 10), 9)

        # Get historical weight for this bin
        historical_weight = normalized_bin_weights[bin_idx]

        # Calculate baseline sample count based on range size
        if range_size < BASE_SAMPLES:
            # For small ranges, scale based on size to avoid redundant sampling
            samples = max(MIN_SAMPLES, range_size)
        else:
            # For larger ranges, use standard allocation
            samples = BASE_SAMPLES

        # Apply historical weight adjustment (blend 70% base, 30% historical)
        samples = int(samples * 0.7 + samples * historical_weight * 0.3)

        # Apply round factor - slight increase for later rounds at same granularity
        round_factor = 1.0 + (state.rounds_at_current_granularity * 0.1)  # 10% increase per round
        samples = int(samples * round_factor)

        # Adjust for previous sampling (avoid resampling already explored areas)
        if range_id_str in state.sampled_regions:
            previous_samples = state.sampled_regions[range_id_str].get("total_samples", 0)
            if previous_samples > 0:
                # Reduce samples if we've explored this range before
                reduction_factor = max(0.5, 1.0 - (previous_samples / (BASE_SAMPLES * 2)))
                samples = int(samples * reduction_factor)
                logging.info(f"Reducing samples for previously explored range {range_id_str} by factor {reduction_factor:.2f}")

        # Special consideration for extreme edges (0-5% and 95-100%) based on historical data
        if range_start_pct < 5 or range_end_pct > 95:
            # These regions have historically contained keys
            samples = int(samples * 1.2)  # 20% bonus

        # Ensure we respect range size and maintain minimum useful sample count
        samples = min(samples, range_size)
        samples = max(MIN_SAMPLES, samples)

        samples_per_range.append(samples)

    # Final safety check: ensure we always have positive samples
    if not samples_per_range or max(samples_per_range or [0]) == 0:
        logging.warning("No positive samples allocated. Using fallback allocation.")
        samples_per_range = [min(BASE_SAMPLES, max(1, r.end - r.start)) for r in ranges_to_evaluate]

    # Log the sampling distribution
    total_allocated = sum(samples_per_range)
    granularity_info = f"Level {state.granularity_level}"
    logging.info(f"Round {state.rounds_at_current_granularity + 1}, {granularity_info}: allocated {total_allocated:,} samples across {len(ranges_to_evaluate)} ranges")

    return samples_per_range

# Statistical evaluation worker
def evaluate_range_statistically(args):
    """
    Statistically evaluates a range of private keys using a progressive refinement approach.
    Enhanced with gradient analysis, bit position significance, and pattern consistency metrics.
    Now includes bit-pattern awareness for Bitcoin puzzles.
    """
    # Handle argument unpacking
    if len(args) > 5:
        range_obj, num_samples_to_take, target_bytes, stat_weights, adaptive_threshold, previously_explored_keys = args
    else:
        range_obj, num_samples_to_take, target_bytes, stat_weights, adaptive_threshold = args
        previously_explored_keys = set()

    s, e = int(range_obj.start), int(range_obj.end)
    current_range_length = e - s
    range_id_str = str((range_obj.start, range_obj.end))

    if current_range_length <= 0 or num_samples_to_take <= 0:
        return None

    # Determine sampling approach based on range size
    if current_range_length <= num_samples_to_take:
        # Range is small enough to test all keys
        logging.info(f"Range {range_id_str} is small enough ({current_range_length:,} keys) to test exhaustively")

        # Filter out previously explored keys
        keys_to_test = set(range(s, e)) - previously_explored_keys

        if not keys_to_test:
            logging.info(f"All keys in range {range_id_str} have been previously explored")
            # Return a placeholder result indicating this range is fully explored
            return RangeStatScore(
                range_id=(s, e),
                score=0.0,
                min_hamming=161,
                avg_hamming=161,
                std_dev_hamming=0.0,
                density_good=0.0,
                best_key_in_range=None,
                num_samples=0,
                entropy=0.0,
                hamming_gradient=0.0,
                bit_significance=0.0,
                pattern_consistency=0.0
            )
    else:
        # Range is larger, use strategic sampling with bit-pattern consideration
        keys_to_test = set()

        # Enhanced sampling strategy considering bit patterns
        # For Bitcoin puzzles, keys have specific bit structure
        step = max(1, current_range_length // num_samples_to_take)

        # Strategy 1: Grid-based sampling with bit-pattern awareness
        for i in range(min(num_samples_to_take // 2, current_range_length)):
            # Grid position
            grid_pos = s + (i * step)

            # Add a small random offset for better coverage
            offset = random.randint(-max(1, step // 20), max(1, step // 20))
            key_pos = grid_pos + offset

            # Ensure position is within range and not previously explored
            key_pos = max(s, min(e-1, key_pos))

            if key_pos not in previously_explored_keys and key_pos not in keys_to_test:
                keys_to_test.add(key_pos)

        # Strategy 2: Add keys with interesting bit patterns
        # Focus on keys where bits alternate or have specific patterns
        remaining_samples = num_samples_to_take - len(keys_to_test)
        if remaining_samples > 0:
            # Sample keys with different bit densities
            for _ in range(remaining_samples):
                # Generate a random position but bias towards certain bit patterns
                if random.random() < 0.3:  # 30% chance for edge cases
                    # Sample near range boundaries
                    if random.random() < 0.5:
                        key_pos = s + random.randint(0, min(1000, current_range_length // 10))
                    else:
                        key_pos = e - random.randint(1, min(1000, current_range_length // 10))
                else:
                    # Regular random sampling
                    key_pos = random.randint(s, e-1)

                if key_pos not in previously_explored_keys and key_pos not in keys_to_test:
                    keys_to_test.add(key_pos)

        logging.info(f"Created strategic sampling with {len(keys_to_test):,} points (avoiding {len(previously_explored_keys):,} previously explored keys)")

    # Calculate hamming distances
    hamming_distances = []
    key_distance_map = {}
    distance_bins = defaultdict(int)

    # For bit-level analysis
    bit_match_counts = [0] * 160  # Count of matches for each bit position across all keys

    # For pattern consistency analysis
    hash160_bit_matrices = []  # Will store bit patterns for consistency analysis

    # For gradient analysis - how hamming distance changes across the range
    key_sorted_list = sorted(list(keys_to_test))
    if len(key_sorted_list) > 1:
        gradient_points = min(20, len(key_sorted_list))  # Use up to 20 points for gradient
        gradient_step = max(1, len(key_sorted_list) // gradient_points)
        gradient_keys = key_sorted_list[::gradient_step][:gradient_points]
    else:
        gradient_keys = key_sorted_list

    gradient_distances = []

    # Bit density analysis for Bitcoin puzzle structure
    bit_densities = []

    for k_val in keys_to_test:
        # Calculate HASH160 of the key
        hash160 = priv_to_hash160_bytes(k_val)
        if hash160 is None:
            continue

        # Calculate Hamming distance
        dist = hamming_distance(hash160, target_bytes)
        key_distance_map[k_val] = dist

        if dist <= 160:
            hamming_distances.append(dist)
            bin_index = dist // 10
            distance_bins[bin_index] += 1

            # Bit-level analysis - which bits match between this hash and target
            bit_matrix = []
            for byte_idx in range(len(hash160)):
                for bit_idx in range(8):
                    hash_bit = (hash160[byte_idx] >> (7 - bit_idx)) & 1
                    target_bit = (target_bytes[byte_idx] >> (7 - bit_idx)) & 1
                    if hash_bit == target_bit:
                        bit_idx_global = byte_idx * 8 + bit_idx
                        if bit_idx_global < 160:  # Ensure we're within 160 bits
                            bit_match_counts[bit_idx_global] += 1
                    # Store bit for pattern analysis
                    bit_matrix.append(1 if hash_bit == target_bit else 0)

            hash160_bit_matrices.append(bit_matrix)

            # Calculate bit density of the key (number of 1 bits)
            bit_density = bin(k_val).count('1')
            bit_densities.append(bit_density)

    # Calculate gradient - how hamming distance changes across the range
    gradient_value = 0
    if len(gradient_keys) > 1:
        for i in range(len(gradient_keys) - 1):
            k1 = gradient_keys[i]
            k2 = gradient_keys[i+1]
            if k1 in key_distance_map and k2 in key_distance_map:
                # Calculate local gradient
                key_diff = k2 - k1
                dist_diff = key_distance_map[k2] - key_distance_map[k1]
                if key_diff > 0:  # Avoid division by zero
                    local_gradient = abs(dist_diff / key_diff)
                    gradient_distances.append(local_gradient)

        # Use average gradient magnitude
        gradient_value = statistics.mean(gradient_distances) if gradient_distances else 0

    if not hamming_distances:
        return None

    min_dist = min(hamming_distances)
    avg_dist = statistics.mean(hamming_distances)
    std_dev_dist = statistics.stdev(hamming_distances) if len(hamming_distances) > 1 else 0.0
    best_key_in_this_range = min(key_distance_map, key=key_distance_map.get)

    # Calculate entropy
    total_samples = sum(distance_bins.values())
    entropy_value = 0
    if total_samples > 0:
        for count in distance_bins.values():
            if count > 0:
                p = count / total_samples
                entropy_value -= p * math.log2(p)

    # Calculate density
    good_keys_count = sum(1 for d in hamming_distances if d < adaptive_threshold)
    density_good_percentage = (good_keys_count / len(hamming_distances)) * 100 if hamming_distances else 0.0

    # NEW: Calculate bit position significance
    # Identify which bit positions tend to match most frequently
    avg_bit_match = sum(bit_match_counts) / (len(bit_match_counts) * len(hamming_distances)) if hamming_distances else 0
    bit_significance_score = 0
    if bit_match_counts and len(hamming_distances) > 0:
        # Calculate weighted score based on how many bits match more often than average
        significant_bits = sum(1 for matches in bit_match_counts if matches > (avg_bit_match * len(hamming_distances)))
        bit_significance_score = significant_bits / 160  # Normalize to 0-1 range

    # NEW: Calculate pattern consistency
    pattern_consistency_score = 0
    if len(hash160_bit_matrices) > 1:
        # Calculate correlation between bit patterns
        correlations = []
        for i in range(min(10, len(hash160_bit_matrices))):
            for j in range(i+1, min(10, len(hash160_bit_matrices))):
                # Calculate correlation between two bit matrices
                pattern_similarity = sum(1 for a, b in zip(hash160_bit_matrices[i], hash160_bit_matrices[j]) if a == b)
                pattern_correlation = pattern_similarity / 160  # Normalize to 0-1
                correlations.append(pattern_correlation)

        pattern_consistency_score = statistics.mean(correlations) if correlations else 0

    # Additional scoring factor based on bit density analysis
    # For Bitcoin puzzles, keys should have approximately N/2 bits set for an N-bit key
    if bit_densities:
        # Expected bit density for uniformly random keys
        puzzle_bits = len(bin(s)) - 2  # Approximate puzzle bit length
        expected_density = puzzle_bits / 2

        # Calculate how close our samples are to expected density
        avg_density = statistics.mean(bit_densities)
        density_deviation = abs(avg_density - expected_density) / expected_density

        # Add a small bonus for samples close to expected density
        if density_deviation < 0.1:  # Within 10% of expected
            bit_significance_score = min(1.0, bit_significance_score * 1.1)

    # Calculate combined score using expanded weights
    score_min_hamming = (160.0 / max(min_dist, 0.1)) * stat_weights.get('min_hamming', 0.65)
    score_density_good = density_good_percentage * stat_weights.get('density_good', 0.30)
    score_avg_hamming = (160.0 / max(avg_dist, 0.1)) * stat_weights.get('avg_hamming', 0.05)
    score_std_dev_penalty = -std_dev_dist * stat_weights.get('std_dev_penalty', 0.0)

    # NEW: Gradient score - lower gradient means we're in a flat region, potentially near solution
    score_gradient = stat_weights.get('hamming_gradient', 0.0) * (1.0 - min(1.0, gradient_value))

    # NEW: Bit significance score
    score_bit_significance = stat_weights.get('bit_significance', 0.0) * bit_significance_score

    # NEW: Pattern consistency score
    score_pattern_consistency = stat_weights.get('pattern_consistency', 0.0) * pattern_consistency_score

    combined_score = max(0.0,
                        score_min_hamming +
                        score_density_good +
                        score_avg_hamming +
                        score_std_dev_penalty +
                        score_gradient +
                        score_bit_significance +
                        score_pattern_consistency)

    # Return with the new metrics included
    return RangeStatScore(
        range_id=(s, e),
        score=combined_score,
        min_hamming=min_dist,
        avg_hamming=avg_dist,
        std_dev_hamming=std_dev_dist,
        density_good=density_good_percentage,
        best_key_in_range=best_key_in_this_range,
        num_samples=len(hamming_distances),
        entropy=entropy_value,
        hamming_gradient=gradient_value,
        bit_significance=bit_significance_score,
        pattern_consistency=pattern_consistency_score
    )

# Range selection strategy
def select_ranges_to_evaluate(state):
    """
    Selects ranges to evaluate using the progressive refinement strategy.
    CLEAN VERSION: No stagnation handling, pure systematic progression.
    """
    # Initialize base ranges if not already defined for current granularity
    if not state.base_ranges:
        state.initialize_base_ranges()

    # Get the next unexplored range at the current granularity level
    next_range = state.get_next_unexplored_range()

    if next_range:
        # Found an unexplored range at this granularity level
        range_id = str((next_range.start, next_range.end))

        # For ultra-fine levels (beyond 1%), subdivide further
        if state.granularity_level > 2:
            # We're at a very fine granularity level (0.1% or finer)
            # Only use ultra-fine exploration for promising ranges
            if range_id in state.range_scores and state.range_scores[range_id].min_hamming < state.best_score + 10:
                # This range has promising scores, explore at ultra-fine granularity
                subranges = subdivide_range_with_granularity(next_range, state.subrange_granularity)
                logging.info(f"Ultra-fine exploration of range {range_id} with granularity {state.subrange_granularity*100:.8f}%")
                return filter_previously_explored_ranges(state, subranges)

        # For standard levels, just return the range as is
        logging.info(f"Exploring range {range_id} at granularity level {state.granularity_level}")
        return [next_range]

    # If all ranges at current level are explored, check if we should advance
    if state.get_exploration_progress() >= 99.0:  # Almost all explored
        logging.info(f"Round {state.rounds_at_current_granularity + 1} completed at granularity level {state.granularity_level}")

        # Advance round at current granularity
        state.rounds_at_current_granularity += 1
        state.total_rounds_completed += 1

        # Check if we should advance to next granularity level
        if state.should_advance_granularity():
            old_level = state.granularity_level
            new_level = state.advance_granularity()
            logging.info(f"✅ Completed 5 rounds at granularity level {old_level}, advancing to level {new_level}")

            # Get the first range at the new granularity level
            next_range = state.get_next_unexplored_range()
            if next_range:
                return [next_range]
        else:
            # Reset exploration state for the next round at current level
            state.thoroughly_explored_ranges.clear()
            state.methodical_exploration_phase = 0
            logging.info(f"Starting round {state.rounds_at_current_granularity + 1} at granularity level {state.granularity_level}")

            # Get the first range for the new round
            next_range = state.get_next_unexplored_range()
            if next_range:
                return [next_range]

    # Fallback: if all else fails, use the first base range
    if state.base_ranges:
        logging.warning(f"No suitable unexplored ranges found. Using first range at level {state.granularity_level}")
        return [state.base_ranges[0]]

    # Final fallback
    logging.error("No valid ranges selected. Using default approach with equal divisions.")
    return create_equal_divisions(state.range_start, state.range_end, 10)

# Helper function for creating equal divisions (used as fallback)
def create_equal_divisions(start, end, num_divisions):
    """Create ranges with equal width divisions"""
    step = max(1, (end - start) // num_divisions)
    ranges = []
    current = start
    while current < end:
        next_end = min(current + step, end)
        if next_end > current:
            ranges.append(Range(current, next_end))
        current = next_end

    # Ensure the last part is covered
    if ranges and ranges[-1].end < end:
        ranges.append(Range(ranges[-1].end, end))
    elif not ranges and end > start:
        ranges.append(Range(start, end))

    return ranges

def subdivide_range_with_granularity(range_obj, granularity_pct):
    """Subdivide a range based on a granularity percentage"""
    range_size = range_obj.end - range_obj.start
    if range_size <= 0:
        return []

    # Calculate how many subdivisions to make
    num_divisions = int(1.0 / granularity_pct)
    sub_size = max(1, range_size // num_divisions)

    sub_ranges = []
    for i in range(num_divisions):
        sub_start = range_obj.start + (i * sub_size)
        sub_end = range_obj.start + ((i + 1) * sub_size)

        # Ensure the last sub-range goes exactly to the end
        if i == num_divisions - 1:
            sub_end = range_obj.end

        if sub_end > sub_start:
            sub_ranges.append(Range(sub_start, sub_end))

    return sub_ranges

def filter_previously_explored_ranges(state, ranges):
    """Filter out ranges that have been thoroughly explored before"""
    filtered_ranges = []

    for r in ranges:
        range_id = str((r.start, r.end))

        # Skip if this exact range was thoroughly explored
        if range_id in state.thoroughly_explored_ranges:
            continue

        # Skip if range is small enough and entirely overlaps with previously explored keys
        range_size = r.end - r.start
        if range_size < 10000:  # Only check for small ranges to avoid memory issues
            all_keys_in_range = set(range(r.start, r.end))
            if all_keys_in_range.issubset(state.previously_explored_keys):
                state.thoroughly_explored_ranges.add(range_id)
                logging.info(f"Range {range_id} skipped - all keys previously explored")
                continue

        # Otherwise keep this range
        filtered_ranges.append(r)

    if not filtered_ranges and ranges:
        # If filtering removed all ranges, return at least one
        return [ranges[0]]

    return filtered_ranges

def report_found_key(found_private_key, current_state):
    hex_private_key = hex(found_private_key)[2:].zfill(64)
    logging.info(f"🎯 EXACT KEY FOUND: {hex_private_key}")

    found_address = priv_to_address(found_private_key)
    if found_address != current_state.target_address:
        logging.error(f"CRITICAL ERROR: Key {hex_private_key} produced address {found_address}, but expected {current_state.target_address}!")
        # This should ideally not happen if brute_force_search confirms the address.

    # Generate WIF (Wallet Import Format) - Uncompressed
    wif_uncompressed = "Error generating WIF"
    try:
        # Prefix for mainnet private key (uncompressed) is 0x80
        pk_bytes_for_wif = b"\x80" + found_private_key.to_bytes(32, "big")
        wif_uncompressed = base58.b58encode_check(pk_bytes_for_wif).decode('ascii')
        logging.info(f"🔑 WIF (Uncompressed): {wif_uncompressed}")
    except Exception as e:
        logging.error(f"Could not generate WIF for key {hex_private_key}: {e}")

    result_filename = os.path.join(DATA_DIR, f"found_key_puzzle{current_state.puzzle_id}.txt")
    try:
        with open(result_filename, "w") as f:
            f.write(f"Puzzle ID: {current_state.puzzle_id}\n")
            f.write(f"Target Bitcoin Address: {current_state.target_address}\n")
            f.write(f"Found Bitcoin Address: {found_address or 'N/A - Check Error Logs'}\n\n")
            f.write(f"Private Key (hex): {hex_private_key}\n")
            f.write(f"Private Key (decimal): {found_private_key}\n")
            f.write(f"Private Key (WIF, Uncompressed): {wif_uncompressed}\n\n")
            f.write(f"Found at (UTC): {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}\n")
            f.write(f"Found in Cycle: {current_state.cycle}\n")
            f.write(f"Final Best Hamming (before finding exact key): {current_state.best_score}\n")
        logging.info(f"Result details saved to {result_filename}")
    except Exception as e:
        logging.error(f"Failed to save result file {result_filename}: {e}")

    # Print to console
    print("\n" + "="*70)
    print("🎉🎉🎉 EXACT KEY FOUND! 🎉🎉🎉".center(70))
    print("="*70)
    print(f"🔑 Private Key (hex): {hex_private_key}")
    print(f"🔑 WIF (Uncompressed): {wif_uncompressed}")
    print(f"🏠 Bitcoin Address: {found_address or 'ERROR - CHECK LOGS'}")
    print(f"💾 Result saved to: {result_filename}")
    print("="*70 + "\n")

    # Attempt to remove checkpoint file as the puzzle is solved
    if os.path.exists(CHECKPOINT_FILE):
        try:
            os.remove(CHECKPOINT_FILE)
            logging.info("Removed checkpoint file as puzzle is solved.")
        except OSError as e:
            logging.error(f"Error removing checkpoint file {CHECKPOINT_FILE}: {e}")

    return hex_private_key, wif_uncompressed # Return some useful info

def print_initial_state(state):
    print("\n" + "=" * 60)
    print(f"🎯 Progressive HASH160 Solver Initializing for Puzzle {state.puzzle_id}")
    print(f"🔑 Target Address: {state.target_address}")
    print(f"🔢 Target HASH160: {TARGET_HASH160_BYTES.hex()}")
    print(f"🔍 Full Search Range: {hex(state.range_start)} -> {hex(state.range_end)}")
    print(f"📏 Total Range Size: {state.range_size:,} keys")
    print(f"🧬 Score Weights: MinH={STAT_SCORE_WEIGHTS['min_hamming']}, Dens={STAT_SCORE_WEIGHTS['density_good']}, AvgH={STAT_SCORE_WEIGHTS['avg_hamming']}")

    # Show granularity level information
    level_name = "10%" if state.granularity_level == 0 else \
                "5%" if state.granularity_level == 1 else \
                "1%" if state.granularity_level == 2 else \
                f"0.{'0' * (state.granularity_level-3)}1%"

    print(f"📋 Current Granularity Level: {state.granularity_level} ({level_name})")
    print(f"📋 Round at Current Level: {state.rounds_at_current_granularity + 1}/5")
    print(f"📋 Total Rounds Completed: {state.total_rounds_completed}")

    # Show exploration progress
    explored_pct = state.get_exploration_progress()
    print(f"📋 Exploration Progress: {explored_pct:.1f}%")

    print(f"🔄 Starting at Cycle {state.cycle}")

    if state.best_key is not None and state.best_score < 161:
        print(f"📈 Resuming with Best Hamming: {state.best_score} (from Key: {hex(state.best_key)})")
        print(f"   Last improvement was at Cycle: {state.last_improvement_cycle}")

    # Show granularity history if available
    if state.granularity_history:
        history_str = ", ".join([f"{g:.2f}%" for g in state.granularity_history])
        print(f"🔍 Granularity History: {history_str}")

    # Show best scores by level if available
    if state.min_hamming_per_level:
        scores_str = ", ".join([str(score) for score in state.min_hamming_per_level])
        print(f"🏆 Best Hamming by Level: {scores_str}")

    # Memory efficiency information
    print(f"🧠 Previously Explored Keys: {len(state.previously_explored_keys):,}")
    print(f"🧠 Memory Efficiency: Avoiding revisiting already checked keys")

    print(f"💾 Checkpoint File: {CHECKPOINT_FILE}")
    print(f"📄 Log File: {LOG_FILE}")
    print("=" * 60 + "\n")

# --- Main Solver Loop ---
def main_statistical_solver(state):
    """
    Main solver loop using the progressive refinement strategy.
    ENHANCED with lattice attack integration.
    """
    print_initial_state(state)

    if state.range_start >= state.range_end:
        logging.critical("Initial range is invalid (start >= end). Exiting.")
        return

    try:
        while True:
            current_cycle_number = state.cycle

            # Progress text based on granularity level
            level_name = "10%" if state.granularity_level == 0 else \
                         "5%" if state.granularity_level == 1 else \
                         "1%" if state.granularity_level == 2 else \
                         f"0.{'0' * (state.granularity_level-3)}1%"

            print(f"\n--- CYCLE {current_cycle_number} (Level {state.granularity_level}: {level_name}, Round {state.rounds_at_current_granularity + 1}, Best Hamming: {state.best_score}) ---")
            logging.info(f"Cycle {current_cycle_number} start. Level: {level_name}, Round: {state.rounds_at_current_granularity + 1}, Best score: {state.best_score}")

            # Update adaptive thresholds based on current best score
            update_adaptive_thresholds(state)

            # Select ranges to evaluate using progressive refinement approach
            candidate_ranges_to_evaluate = select_ranges_to_evaluate(state)

            if not candidate_ranges_to_evaluate:
                logging.error("No candidate ranges selected. Exiting.")
                break

            # Determine sample counts for selected ranges
            total_samples_budget = FIXED_SAMPLE_COUNT * len(candidate_ranges_to_evaluate)
            current_cycle_sample_counts = allocate_samples_based_on_weights(
                state, candidate_ranges_to_evaluate, total_samples_budget)

            # Create pool arguments, including previously explored keys
            pool_args_list = [
                (r_obj, s_count, TARGET_HASH160_BYTES, STAT_SCORE_WEIGHTS, state.adaptive_hamming_threshold, state.previously_explored_keys)
                for r_obj, s_count in zip(candidate_ranges_to_evaluate, current_cycle_sample_counts)
                if s_count > 0
            ]

            if not pool_args_list:
                logging.warning("No ranges with positive sample counts. Advancing cycle.")
                state.cycle += 1
                save_checkpoint(state)
                time.sleep(1)
                continue

            evaluated_range_results = {}
            num_cores_for_stat_eval = max(1, mp.cpu_count())
            logging.info(f"Evaluating {len(pool_args_list)} ranges using {num_cores_for_stat_eval} cores.")

            with mp.Pool(processes=num_cores_for_stat_eval) as pool:
                async_stat_results = [pool.apply_async(evaluate_range_statistically, args=(args,)) for args in pool_args_list]
                pool.close()

                # Track previous best score to detect improvements
                improvement_detected = False

                for i, async_res in enumerate(async_stat_results, 1):
                    try:
                        range_stat_score_obj = async_res.get()

                        if range_stat_score_obj:
                            range_id_key_str = str(range_stat_score_obj.range_id)
                            evaluated_range_results[range_id_key_str] = range_stat_score_obj

                            # Update the sample count for this range
                            if range_id_key_str not in state.range_samples_count:
                                state.range_samples_count[range_id_key_str] = 0
                            state.range_samples_count[range_id_key_str] += range_stat_score_obj.num_samples

                            # Mark range as thoroughly explored if needed
                            state.mark_range_explored(Range(*range_stat_score_obj.range_id), range_stat_score_obj.num_samples)

                            # Check for improvement in Hamming distance
                            if range_stat_score_obj.min_hamming < state.best_score:
                                state.best_score = range_stat_score_obj.min_hamming
                                state.best_key = range_stat_score_obj.best_key_in_range
                                state.last_improvement_cycle = current_cycle_number
                                improvement_detected = True
                                logging.info(f"🎉 NEW GLOBAL BEST: Hamming {state.best_score} from key {hex(state.best_key)} in range {range_id_key_str}")
                                print(f"   🏆 New best Hamming: {state.best_score}!")

                                # If we've found the exact key (Hamming = 0), report it and exit
                                if state.best_score == 0:
                                    if priv_to_address(state.best_key) == TARGET_ADDRESS:
                                        report_found_key(state.best_key, state)
                                        logging.info("Exact key found and reported. Terminating solver.")
                                        return

                        print(f"   Evaluation progress: {i}/{len(async_stat_results)} ranges processed.", end='\r')
                    except Exception as e:
                        logging.error(f"Error processing evaluation result for task {i}: {e}", exc_info=True)


            # Update state with evaluation results
            for range_id_key_str, stat_score in evaluated_range_results.items():
                # Store the range score for future reference
                state.range_scores[range_id_key_str] = stat_score

                # Store entropy data for future sampling decisions
                if hasattr(stat_score, 'entropy'):
                    state.region_entropy_map[range_id_key_str] = stat_score.entropy

                # Record sampled keys for future reference
                if stat_score.best_key_in_range is not None:
                    try:
                        range_tuple = eval(range_id_key_str)
                        state.record_sampled_keys(range_id_key_str, [stat_score.best_key_in_range])
                    except Exception as e:
                        logging.error(f"Error recording sampled keys for {range_id_key_str}: {e}")

            if evaluated_range_results:
                # Update exploration progress display
                progress = state.get_exploration_progress()
                print(f"   Level {state.granularity_level} ({level_name}), Round {state.rounds_at_current_granularity + 1}: {progress:.1f}% explored")

            # Display enhanced metrics for the best ranges
            if evaluated_range_results:
                best_ranges = sorted(evaluated_range_results.values(), key=lambda rs: rs.score, reverse=True)[:3]
                logging.info(f"Top 3 ranges with enhanced metrics:")
                for i, rs in enumerate(best_ranges, 1):
                    if hasattr(rs, 'hamming_gradient') and hasattr(rs, 'bit_significance') and hasattr(rs, 'pattern_consistency'):
                        logging.info(f"  Range #{i}: score={rs.score:.2f}, min_hamming={rs.min_hamming}, " +
                                    f"gradient={rs.hamming_gradient:.6f}, bit_sig={rs.bit_significance:.2f}, " +
                                    f"pattern_con={rs.pattern_consistency:.2f}")

                        # Print a more detailed analysis for the very best range
                        if i == 1 and rs.min_hamming < 50:
                            print(f"   🔍 Enhanced analysis of best range: {rs.range_id}")
                            print(f"     Hamming Distance: {rs.min_hamming} (min) / {rs.avg_hamming:.1f} (avg)")
                            print(f"     Gradient: {rs.hamming_gradient:.6f} - {'LOW (good)' if rs.hamming_gradient < 0.001 else 'MODERATE' if rs.hamming_gradient < 0.01 else 'HIGH'}")
                            print(f"     Bit Significance: {rs.bit_significance:.2f} - {'HIGH (good)' if rs.bit_significance > 0.6 else 'MODERATE' if rs.bit_significance > 0.4 else 'LOW'}")
                            print(f"     Pattern Consistency: {rs.pattern_consistency:.2f} - {'HIGH (good)' if rs.pattern_consistency > 0.7 else 'MODERATE' if rs.pattern_consistency > 0.5 else 'LOW'}")

            # Increment cycle and save checkpoint
            state.cycle += 1
            save_checkpoint(state)

    except KeyboardInterrupt:
        print("\n⏹️ Solver interrupted by user. Saving checkpoint...")
        save_checkpoint(state)
        print("Checkpoint saved. Exiting.")
    except Exception as e:
        logging.critical(f"Fatal error in main solver loop: {e}", exc_info=True)
        print(f"\n❌ A fatal error occurred: {e}. Saving checkpoint...")
        save_checkpoint(state)
        print("Checkpoint saved. Please check logs for details.")

if __name__ == "__main__":
    try:
        # Set up multiprocessing
        start_method = 'spawn' if os.name == 'nt' else 'fork'
        if mp.get_start_method(allow_none=True) != start_method:
            mp.set_start_method(start_method, force=True)
            logging.info(f"Multiprocessing start method set to '{start_method}'.")
    except Exception as e:
        logging.warning(f"Could not set multiprocessing start method: {e}")

    # Load checkpoint or create new state
    current_search_state = load_checkpoint()

    # Ensure base ranges are initialized for current granularity
    if not current_search_state.base_ranges:
        current_search_state.initialize_base_ranges()
        logging.info(f"Initialized base ranges for granularity level {current_search_state.granularity_level}")

    # Always use breadth mode for progressive approach
    current_search_state.current_exploration_mode = "breadth"

    # Check if resuming from a previous run
    if current_search_state.cycle > 1:
        level_name = "10%" if current_search_state.granularity_level == 0 else \
                    "5%" if current_search_state.granularity_level == 1 else \
                    "1%" if current_search_state.granularity_level == 2 else \
                    f"0.{'0' * (current_search_state.granularity_level-3)}1%"

        logging.info(f"🔄 Resuming progressive search from cycle {current_search_state.cycle}")
        logging.info(f"📊 Current granularity level: {current_search_state.granularity_level} ({level_name})")
        logging.info(f"🔢 Round at current level: {current_search_state.rounds_at_current_granularity + 1}")
        logging.info(f"🧩 Puzzle bit structure: {current_search_state.puzzle_bit_length} bits with {current_search_state.leading_zeros_bits} leading zeros")

        if current_search_state.best_key:
            logging.info(f"🏆 Best Hamming distance so far: {current_search_state.best_score}")
            # Log the best key with proper hex format showing leading zeros
            best_key_hex = hex(current_search_state.best_key)[2:].zfill(current_search_state.puzzle_bit_length // 4 + 1)
            full_hex = current_search_state.hex_prefix + best_key_hex
            logging.info(f"🔑 Best key so far: {full_hex}")

        # Show memory efficiency stats
        logging.info(f"🧠 Previously explored keys: {len(current_search_state.previously_explored_keys):,}")

    # Validate puzzle configuration
    if hasattr(current_search_state, 'puzzle_bit_length'):
        expected_start = 1 << (current_search_state.puzzle_bit_length - 1)
        expected_end = (1 << current_search_state.puzzle_bit_length) - 1

        if current_search_state.range_start == expected_start and current_search_state.range_end == expected_end:
            logging.info(f"✅ Range configuration matches puzzle #{current_search_state.puzzle_bit_length} structure")
        else:
            logging.warning(f"⚠️ Range mismatch for puzzle #{current_search_state.puzzle_bit_length}")
            logging.warning(f"  Expected: [{hex(expected_start)}, {hex(expected_end)}]")
            logging.warning(f"  Actual:   [{hex(current_search_state.range_start)}, {hex(current_search_state.range_end)}]")


    logging.info("🚀 Starting STATISTICAL SOLVER ")

    main_statistical_solver(current_search_state)